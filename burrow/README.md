
Burrow
===========

A Helm chart for burrow (https://github.com/linkedin/Burrow)


## Configuration

The following table lists the configurable parameters of the Burrow chart and their default values.

| Parameter                | Description             | Default        |
| ------------------------ | ----------------------- | -------------- |
| `replicaCount` |  | `1` |
| `image.repository` |  | `"magnetme/burrow"` |
| `image.tag` |  | `"latest"` |
| `image.pullPolicy` |  | `"IfNotPresent"` |
| `imagePullSecrets` |  | `[]` |
| `nameOverride` |  | `""` |
| `fullnameOverride` |  | `""` |
| `serviceAccount.create` |  | `true` |
| `serviceAccount.annotations` |  | `{}` |
| `serviceAccount.name` |  | `""` |
| `podAnnotations` |  | `{}` |
| `podSecurityContext` |  | `{}` |
| `securityContext` |  | `{}` |
| `burrow.command.path` |  | `"/go/bin/burrow"` |
| `resources` |  | `{}` |
| `nodeSelector` |  | `{}` |
| `tolerations` |  | `[]` |
| `affinity` |  | `{}` |
| `service.type` |  | `"ClusterIP"` |
| `service.port` |  | `8000` |
| `readinessProbe.enable` |  | `true` |
| `readinessProbe.periodSeconds` |  | `60` |
| `readinessProbe.timeoutSeconds` |  | `3` |
| `readinessProbe.initialDelaySeconds` |  | `10` |
| `livenessProbe.enable` |  | `true` |
| `livenessProbe.periodSeconds` |  | `60` |
| `livenessProbe.timeoutSeconds` |  | `1` |
| `livenessProbe.initialDelaySeconds` |  | `30` |
| `config.burrow.cfg` |  | `"[general]\nlogconfig=/etc/burrow/logging.cfg\ngroup-blacklist=^(console-consumer-|python-kafka-consumer-).*$\n\n[zookeeper]\nhostname=zookeeper\nport=2181\ntimeout=6\nlock-path=/burrow/notifier\n\n[kafka \"local\"]\nbroker=kafka\nbroker-port=9092\noffsets-topic=__consumer_offsets\nzookeeper=zookeeper\nzookeeper-path=/local\nzookeeper-offsets=true\noffsets-topic=__consumer_offsets\n\n[tickers]\nbroker-offsets=60\n\n[lagcheck]\nintervals=10\nexpire-group=604800\n\n[httpserver]\nserver=on\nport=8000\n"` |
| `config.logging.cfg` |  | `"<seelog minlevel=\"debug\">\n  <outputs formatid=\"main\">\n    <rollingfile type=\"date\" filename=\"/var/tmp/burrow/burrow.log\" datepattern=\"2006-01-02\" maxrolls=\"7\" />\n  </outputs>\n  <formats>\n    <format id=\"main\" format=\"%Date(2006-01-02 15:04:05) [%LEVEL] %Msg%n\"/>\n  </formats>\n</seelog>\n"` |
| `datadog.enabled` |  | `false` |
| `datadog.apiKey` |  | `""` |
| `datadog.logLevel` |  | `"INFO"` |
| `datadog.image.repository` |  | `"datadog/agent"` |
| `datadog.image.tag` |  | `7` |
| `datadog.image.pullPolicy` |  | `"Always"` |
| `datadog.image.resources` |  | `null` |
| `datadog.checks_d.clusters` |  | `["local"]` |
| `datadog.checks_d.burrowCheck` |  | `"# stdlib\nfrom urllib.parse import urljoin\n\n# 3rd Party\nimport requests\nimport json\n\ntry:\n    from datadog_checks.base import AgentCheck\nexcept ImportError:\n    from checks import AgentCheck\n\n__version__ = \"1.0.0\"\n\n# project\nSERVICE_CHECK_NAME = 'burrow.can_connect'\n# appname burrow, namespace: burrow\nDEFAULT_BURROW_URI = 'http://burrow.burrow:8000'\nCLUSTER_ENDPOINT = '/v2/kafka'\nCHECK_TIMEOUT = 10\nclass BurrowCheck(AgentCheck):\n  '''\n  Extract consumer offsets, topic offsets and offset lag from Burrow REST API\n  '''\n  def check(self, instance):\n      burrow_address = instance.get(\"burrow_uri\", DEFAULT_BURROW_URI)\n      target_clusters = instance.get(\"clusters\")\n      extra_tags = instance.get(\"tags\", [])\n      self._check_burrow(burrow_address, extra_tags)\n      clusters = self._find_clusters(burrow_address, target_clusters)\n      self.log.debug(\"Collecting Topic Offsets\")\n      self._topic_offsets(clusters, burrow_address, extra_tags)\n      self.log.debug(\"Collecting Consumer Group Offsets\")\n      self._consumer_groups_offsets(clusters, burrow_address, extra_tags)\n      self.log.debug(\"Collecting Consumer Group lags\")\n      self._consumer_groups_lags(clusters, burrow_address, extra_tags)\n\n  def _consumer_groups_lags(self, clusters, burrow_address, extra_tags):\n      \"\"\"\n      Retrieve the offsets for all consumer groups in the clusters\n      Getting Consumer list could be factored out\n      \"\"\"\n      for cluster in clusters:\n          consumers_path = \"%s/%s/consumer\" % (CLUSTER_ENDPOINT, cluster)\n          consumers_list = self._rest_request_to_json(burrow_address, consumers_path).get(\"consumers\", [])\n          for consumer in consumers_list:\n              lags_path = \"%s/%s/lag\" % (consumers_path, consumer)\n              lag_json = self._rest_request_to_json(burrow_address, lags_path)\n              if not lag_json:\n                  continue\n              status = lag_json[\"status\"]\n              consumer_tags = [\"cluster:%s\" % cluster, \"consumer:%s\" % consumer] + extra_tags\n              self.gauge(\"kafka.consumer.maxlag\", status[\"maxlag\"], tags=consumer_tags)\n              self.gauge(\"kafka.consumer.totallag\", status[\"totallag\"], tags=consumer_tags)\n              self._submit_lag_status(\"kafka.consumer.lag_status\", status[\"status\"], tags=consumer_tags)\n              for partition in status.get(\"partitions\", []):\n                  partition_tags = consumer_tags + [\"topic:%s\" % partition[\"topic\"], \"partition:%s\" % partition[\"partition\"]]\n                  self._submit_partition_lags(partition, partition_tags)\n                  self._submit_lag_status(\"kafka.consumer.partition_lag_status\", partition[\"status\"], tags=partition_tags)\n  def _submit_lag_status(self, metric_namespace, status, tags):\n      burrow_status = {\n              \"UNKNOWN\" : 0,\n              \"OK\": 0,\n              \"WARN\": 0,\n              \"ERR\": 0,\n              \"STOP\": 0,\n              \"STALL\": 0,\n              \"REWIND\": 0\n      }\n      if status not in list(burrow_status.keys()):\n          self.log.error(\"Invalid lag status: '%s' for '%s'\" % (status, tags))\n          return\n      burrow_status[status] = 1\n      for metric_name, value in burrow_status.items():\n          self.gauge(\"%s.%s\" % (metric_namespace, metric_name.lower()), value, tags=tags)\n  def _submit_partition_lags(self, partition, tags):\n      lag = partition.get(\"end\").get(\"lag\")\n      timestamp = partition.get(\"end\").get(\"timestamp\") / 1000\n      self.gauge(\"kafka.consumer.partition_lag\", lag, tags=tags)\n  def _check_burrow(self, burrow_address, extra_tags):\n      \"\"\"\n      Check the Burrow health endpoint\n      \"\"\"\n      url = urljoin(burrow_address, \"/burrow/admin\")\n      try:\n          tags = ['instance:%s' % self.hostname] + extra_tags\n          response = requests.get(url, timeout=CHECK_TIMEOUT)\n          response.raise_for_status()\n      except Exception as e:\n          self.service_check(SERVICE_CHECK_NAME,\n                             AgentCheck.CRITICAL, tags=tags,\n                             message=str(e))\n          raise\n      else:\n          self.service_check(SERVICE_CHECK_NAME, AgentCheck.OK,\n                             tags=tags,\n                             message='Connection to %s was successful' % url)\n  def _topic_offsets(self, clusters, burrow_address, extra_tags):\n      \"\"\"\n      Retrieve the offsets for all topics in the clusters\n      \"\"\"\n      for cluster in clusters:\n          cluster_path = \"%s/%s\" % (CLUSTER_ENDPOINT, cluster)\n          offsets_topic = self._rest_request_to_json(burrow_address, cluster_path)[\"cluster\"][\"offsets_topic\"]\n          topics_path = \"%s/topic\" % cluster_path\n          topics_list = self._rest_request_to_json(burrow_address, topics_path).get(\"topics\", [])\n          for topic in topics_list:\n              if topic == offsets_topic:\n                  continue\n              topic_path = \"%s/%s\" % (topics_path, topic)\n              response = self._rest_request_to_json(burrow_address, topic_path)\n              tags = [\"topic:%s\" % topic, \"cluster:%s\" % cluster] + extra_tags\n              self._submit_offsets_from_json(offsets_type=\"topic\", json=response, tags=tags)\n  def _consumer_groups_offsets(self, clusters, burrow_address, extra_tags):\n      \"\"\"\n      Retrieve the offsets for all consumer groups in the clusters\n      \"\"\"\n      for cluster in clusters:\n          consumers_path = \"%s/%s/consumer\" % (CLUSTER_ENDPOINT, cluster)\n          consumers_list = self._rest_request_to_json(burrow_address, consumers_path).get(\"consumers\", [])\n          for consumer in consumers_list:\n              topics_path = \"%s/%s/topic\" % (consumers_path, consumer)\n              topics_list = self._rest_request_to_json(burrow_address, topics_path).get(\"topics\", [])\n              for topic in topics_list:\n                  topic_path = \"%s/%s\" % (topics_path, topic)\n                  response = self._rest_request_to_json(burrow_address, topic_path)\n                  if not response:\n                      continue\n                  tags = [\"topic:%s\" % topic, \"cluster:%s\" % cluster,\n                          \"consumer:%s\" % consumer] + extra_tags\n                  self._submit_offsets_from_json(offsets_type=\"consumer\", json=response, tags=tags)\n  def _submit_offsets_from_json(self, offsets_type, json, tags):\n      \"\"\"\n      Find the offsets and push them into the metrics\n      \"\"\"\n      offsets = json.get(\"offsets\")\n      if offsets:\n          # for unconsumed or empty partitions, change an offset of -1 to 0 so the\n          # sum isn't affected by the number of empty partitions.\n          offsets = [max(offset, 0) for offset in offsets]\n          self.gauge(\"kafka.%s.offsets.total\" % offsets_type, sum(offsets), tags=tags)\n          for partition_number, offset in enumerate(offsets):\n              new_tags = tags + [\"partition:%s\" % partition_number]\n              self.gauge(\"kafka.%s.offsets\" % offsets_type, offset, tags=new_tags)\n  def _find_clusters(self, address, target):\n      \"\"\"\n      Find the available clusters in Burrow, return all clusters if\n      target is not set.\n      \"\"\"\n      available_clusters = self._rest_request_to_json(address, CLUSTER_ENDPOINT).get(\"clusters\")\n      if not available_clusters:\n          raise Exception(\"There are no clusters in Burrow\")\n      if not target:\n          return available_clusters\n      else:\n          clusters = []\n          for name in target:\n              if name in available_clusters:\n                  clusters.append(name)\n              else:\n                  self.log.error(\"Cluster '%s' does not exist\" % name )\n          return clusters\n  def _rest_request_to_json(self, address, object_path):\n      '''\n      Query the given URL and return the JSON response\n      '''\n      response_json = None\n      service_check_tags = ['instance:%s' % self.hostname]\n      url = urljoin(address, object_path)\n      try:\n          response = requests.get(url)\n          response.raise_for_status()\n          response_json = response.json()\n          if response_json[\"error\"]:\n              self.log.error(\"Burrow Request failed: %s: %s\" % (object_path, response_json[\"message\"]))\n              return {}\n      except requests.exceptions.Timeout as e:\n          self.log.error(\"Request timeout: {0}, {1}\".format(url, e))\n          raise\n      except (requests.exceptions.HTTPError,\n              requests.exceptions.InvalidURL,\n              requests.exceptions.ConnectionError) as e:\n          self.log.error(\"Request failed: {0}, {1}\".format(url, e))\n          raise\n      except ValueError as e:\n          self.log.error(str(e))\n          raise\n      else:\n          self.log.debug('Connection to %s was successful' % url)\n      return response_json\n"` |



---
_Documentation generated by [Frigate](https://frigate.readthedocs.io)._

